% Discuss the scientific literature that addresses the same or a similar problem as your study, and make clear how your study relates and adds to this body of research. Try to give a broad but concise overview of the ways in which the problem is approached in other studies. 

\section{Related Work}

The link between typological similarity and successful cross-lingual transfer is well established. Pires et al.~\cite{pires-etal-2019-multilingual} found that typological similarity correlates positively with zero-shot learning performance. While they attributed their results to lexical overlap between the source and target languages, Karthikeyan et al.~\cite{karthikeyan-etal-2020-cross} found the same results when the source and target languages had no lexical overlap at all. Instead, they demonstrated structural similarity between two languages to be of greater importance.

Typological relatedness has informed cross-lingual research in low-resource settings, too. Nooralahzadeh et al.~\cite{nooralahzadeh-etal-2020-zero} use a multi-stage approach to cross-lingual transfer with MAML where between the initial stage of fine-tuning on a large dataset to instill task-specific knowledge and few-shot learning to low-resource languages, they fine-tune on auxiliary languages to optimize the model parameters for quick adaptation. Choenni et al.~\cite{choenni-etal-2023-cross} adapt this method and match auxiliary languages and low-resource languages based on typological similarity to further facilitate cross-lingual transfer.

Tenney et al.~\cite{tenney-etal-2019-bert} set out to quantify where specific types of linguistic information are encoded in BERT, and found each consecutive layer to handle incrementally high-level linguistic information. That is, earlier layers handle concrete features like parts-of-speech tagging, whereas more abstract information like dependency relations and coreference resolution is dealt with in later layers. In the multilingual setting, Choenni \& Shutova~\cite{choenni-shutova-2022-investigating} found that M-BERT's internal representations have a clear typological organization, with the model encoding typological features explicitly and jointly across languages.

This joint encoding could be what underlies the relation between typological similarity and successful cross-lingual transfer. Fine-tuning an MLM on any given language could see it increase the importance of the encodings of the typological features of that language. As a result, after fine-tuning, the model's parameters would be close to what they would have been if the model had been fine-tuned to a typologically similar language.
